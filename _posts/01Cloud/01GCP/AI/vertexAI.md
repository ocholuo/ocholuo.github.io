---
title: GCP  - Vertex AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [01GCP]
# img: /assets/img/sample/rabbit.png
tags: [AI, ML]
---

# Vertex AI

- [Vertex AI](#vertex-ai)
  - [Overview](#overview)
    - [Codey](#codey)
      - [Getting Started with Codey](#getting-started-with-codey)
      - [Code Generation with Codey](#code-generation-with-codey)
      - [Code Chat with Codey](#code-chat-with-codey)
  - [command](#command)
    - [model call](#model-call)
      - [crul](#crul)
      - [py](#py)

ref:
- [cthesera - gcp-big-data-ml-fundamentals](https://www.cthesera.org/learn/gcp-big-data-ml-fundamentals)

---

## Overview

- OpenAI has released countless Generative AI and Large Language Models built on top of their top-tier GPT frameworks, including ChatGPT, their Generative Conversational AI.
- After the successful creation of conversational language models, developers are constantly trying to create Large Language Models that can either develop or assist developers in coding applications.
- Many companies have started researching these LLMs, including OpenAI, that would help developers build applications faster with the LLMs knowing programming languages.
- Google built Codey, a fine-tuned model of PaLM 2, capable of performing varying coding tasks.


### Codey

- one of the `foundational models` built and released by Google
  - available to the general public through Vertex AI in the Google Cloud Platform.
  - Google has recently made available the recently announced Google Foundational models, which include `PaLM 2, Codey, Chirp, and Imagen`.

- based on the `PaLM 2 Large Language Model`.
  - Codey is a fine-tuned model of the PaLM 2 Large Language Model.
  - A large corpus of high-quality codes and coding documents has fine-tuned Codey.
  - Google claims that Codey can code in more than 20+ programming languages, including Python, C, Javascript, Java, and more.
  - Codey was used to enhance Google products like Google Colab, Android Studio, etc.

- Codey is built to solve 3 purposes.

  - **code completion**:
    - analyze the writing code and make valuable suggestions based on it.
    - it is context-aware of the code you are writing.

  - **code generation**.
    - generate complete workable code in any language, provided the prompt.

  - **chat with the code**: provide the code to Codey and chat with Codey related to the code.


#### Getting Started with Codey

To work with Google’s Codey
- have an account with the Google Cloud Platform.
- enable the Vertex AI API to work with Vertex AI.
  - go to the API & Services -> Library -> Vertex AI API -> “Enable API”


#### Code Generation with Codey

**Prerequisite**
- enabling the Vertex AI API in the GCP
- The code walkthrough here will take place in Google Colab.
- install some necessary packages to work with Vertex AI
  - The `Shapley` and the `google-cloud-aiplatform` are the only two required packages to start working with the Codey model.

```bash
!pip install shapely
!pip install google-cloud-aiplatform>=1.27.0
```

- authenticate the Google account, so Colab can use the GCP credentials to run the Codey model from Vertex AI.

```py
from google.colab import auth as google_auth
google_auth.authenticate_user()
# import the google_auth from Google.colab package.
# authenticate by allowing the Colab to use the credentials for running the Codey model from Vertex AI.

import vertexai
from vertexai.preview.language_models import CodeGenerationModel
# import the vertex, the package containing all the machine learning and AI-related models composed by Google

vertexai.init(
  project="the_project_id", location="us-west1")
parameters = {
    "temperature": 0.3,
    # how creative the model should be
    "max_output_tokens": 1024
    # the limit set to the length of the output generated by the LLM
}
```

take this imported model, i.e., the CodeGenerationModel, and test it by passing a prompt.

**Prompt**

```py
code_model = CodeGenerationModel.from_pretrained("code-bison@001")
response = code_model.predict(
    prefix = """
    Write a code in Python to count the occurrence of the
    word "rocket" from a given input sentence using Regular Expressions
    """,
    **parameters
)
```



#### Code Chat with Codey

- The Code Chat function allows us to interact with Codey on the code.
- We provide the Code to Codey and chat with the Codey model about the code.
- It can be either to understand better the code, like how it works, or if we want alternate approaches for the given code, which Codey can do by looking at the current code.
- If we face any errors, then we may provide both the code and the error, which Codey will look at and give a solution to solve the error.

example
- introduce an error to the Regular Expression code. In the Python Regex code, replace the `re.findall()` with `re.find()` and run the code
- the Codey model has analyzed the code and suggested where the error was. It even provided the corrected code for us to work with.

---

## command


### model call

#### crul

```bash
cat << EOF > request.json
{
    "contents": [
    ]
    , "generationConfig": {
        "temperature": 1
        ,"maxOutputTokens": 8192
        ,"topP": 0.95
    },
    "safetySettings": [
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "OFF"
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "OFF"
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "OFF"
        },
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "OFF"
        }
    ]
}
EOF

# base
PROJECT_ID="PROJECT_ID"
LOCATION_ID="us-central1"
API_ENDPOINT="us-central1-aiplatform.googleapis.com"
MODEL_ID="gemini-1.5-pro-002"

curl \
-X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION_ID}/publishers/google/models/${MODEL_ID}:streamGenerateContent" -d '@request.json'


# FT
PROJECT_ID="PROJECT_ID"
LOCATION_ID="us-central1"
API_ENDPOINT="us-central1-aiplatform.googleapis.com"
MODEL_ID="projects/PROJECT_ID/locations/us-central1/endpoints/endpoints_id"

curl \
-X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://${API_ENDPOINT}/v1/${MODEL_ID}:streamGenerateContent" -d '@request.json'
```

#### py

```py
import base64
import vertexai
from vertexai.generative_models import GenerativeModel, SafetySetting, Part

def multiturn_generate_content():
    vertexai.init(
        project="project_id",
        location="region_name",
    )
    model = GenerativeModel(
        "gemini-1.5-pro-002",
        "projects/project_id/locations/region_name/endpoints/endpoints_id",
    )
    chat = model.start_chat()


generation_config = {
    "max_output_tokens": 8192,
    "temperature": 1,
    "top_p": 0.95,
}

safety_settings = [
    SafetySetting(
        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold=SafetySetting.HarmBlockThreshold.OFF
    ),
    SafetySetting(
        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=SafetySetting.HarmBlockThreshold.OFF
    ),
    SafetySetting(
        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold=SafetySetting.HarmBlockThreshold.OFF
    ),
    SafetySetting(
        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold=SafetySetting.HarmBlockThreshold.OFF
    ),
]

multiturn_generate_content()
```


```py
from google import genai
from google.genai import types
import base64

def generate():
  client = genai.Client(
      vertexai=True,
      project="project_name",
      location="region_name",
  )

  model = "gemini-2.0-flash-exp"
  contents = []
  generate_content_config = types.GenerateContentConfig(
    temperature = 1,
    top_p = 0.95,
    max_output_tokens = 8192,
    response_modalities = ["TEXT"],
    safety_settings = [
      types.SafetySetting(
        category="HARM_CATEGORY_HATE_SPEECH",
        threshold="OFF"
      ),
      types.SafetySetting(
        category="HARM_CATEGORY_DANGEROUS_CONTENT",
        threshold="OFF"
      ),
      types.SafetySetting(
        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
        threshold="OFF"
      ),types.SafetySetting(
        category="HARM_CATEGORY_HARASSMENT",
        threshold="OFF"
      )
    ],
  )

  for chunk in client.models.generate_content_stream(
    model = model,
    contents = contents,
    config = generate_content_config,
    ):
    print(chunk.text, end="")

generate()
```
